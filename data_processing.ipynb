{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37664bitbaseconda80ebf1b4e8ae42f78b2b9972a4e4db4a",
   "display_name": "Python 3.7.6 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import music21\n",
    "from tqdm import tqdm\n",
    "\n",
    "import utils"
   ]
  },
  {
   "source": [
    "# Collecting Data from MIDI files"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "here, i get the sequence of notes and their duration for each song in a dataset of classical music"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "midi_files = utils.get_midi_files('data\\\\classical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['data\\\\classical\\\\adam\\\\3000adamno_l.mid',\n 'data\\\\classical\\\\adam\\\\3002robdia.mid',\n 'data\\\\classical\\\\adam\\\\3003marquise.mid',\n 'data\\\\classical\\\\aguado\\\\AguadoLessonNo1.mid',\n 'data\\\\classical\\\\aguado\\\\AguadoMenuet1Op12.mid']"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "midi_files[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "3029"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "len(midi_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "100%|██████████| 2/2 [00:11<00:00,  5.66s/it]\n"
    }
   ],
   "source": [
    "songs_notes = []\n",
    "\n",
    "for midi_file in tqdm(midi_files):\n",
    "    \n",
    "    mid = utils.parse_midi_file(midi_file)\n",
    "\n",
    "    if mid is None: continue\n",
    "\n",
    "    # pitches of song i.e.: C#, A, G, etc...durations ignored for now\n",
    "    notes = np.array(utils.get_notes(mid, chord_root=True, duration_type=True))\n",
    "    \n",
    "    songs_notes.append(notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "2"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "len(songs_notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs_notes = np.array(songs_notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(619, 2)"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "songs_notes[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('processed_data/classical_songs_notes.npy', songs_notes)"
   ]
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Vectorize Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "vectorize data to a format suitable for training a generative model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('processed_data/classical_songs_notes.npy', 'rb') as data:\n",
    "    data = np.load(data, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "song = data[0]"
   ]
  },
  {
   "source": [
    "data is currently of the form of a sequence of pitch-duration pairs where pitches 0-11 correspond to notes C-B and durations are strings of the duration type"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([['2', '16th'],\n       ['2', 'half'],\n       ['2', '16th'],\n       ...,\n       ['5', 'half'],\n       ['10', 'complex'],\n       ['10', 'half']], dtype='<U11')"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "song"
   ]
  },
  {
   "source": [
    "the song is mapped to a sequence of a unique integer class for each pitch-duration pair "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(619, 2)"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "song.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_pitches, song_durs = utils.map_song(song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "((619,), (619,))"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "song_pitches.shape, song_durs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[ 2  2  2  5  5  7  7  3  7 10  5  5  2  0 10  2  3  5  3  0]\n[0 3 0 3 0 2 0 2 0 3 1 0 2 0 2 2 0 2 2 0]\n"
    }
   ],
   "source": [
    "print(song_pitches[:20])\n",
    "print(song_durs[:20])"
   ]
  },
  {
   "source": [
    "now do this for all songs"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "100%|██████████| 3013/3013 [00:19<00:00, 154.19it/s]\n"
    }
   ],
   "source": [
    "vec_songs = []\n",
    "\n",
    "for song in tqdm(data):\n",
    "    vec_songs.append(utils.map_song(song))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_songs = np.array(vec_songs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('processed_data/vectorized_classical_songs2.npy', vec_songs)"
   ]
  }
 ]
}